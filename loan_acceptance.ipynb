{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a2e477f-58b3-4521-a371-591aa8e35efd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder, Imputer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# Read CSV file\n",
    "df = spark.sql(\"SELECT * FROM dbacademy.labuser9258060_1739805221.loan_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea4352cd-e365-4848-a97a-efe2ecb71fc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Impute rows with missing values\n",
    "df = df.fillna(0)\n",
    "\n",
    "# List of categorical columns to be encoded\n",
    "categorical_columns = [\"person_education\", \"person_home_ownership\", \"loan_intent\", \"previous_loan_defaults_on_file\", \"loan_status\"]\n",
    "\n",
    "# List of numerical columns\n",
    "numerical_columns = [\"person_age\", \"person_income\", \"person_emp_exp\", \"loan_amnt\", \"loan_int_rate\", \"loan_percent_income\", \"cb_person_cred_hist_length\", \"credit_score\"]\n",
    "\n",
    "for column in numerical_columns:\n",
    "    df = df.withColumn(column, col(column).cast(\"float\"))\n",
    "\n",
    "# Create a StringIndexer and OneHotEncoder for each categorical column\n",
    "indexers = [StringIndexer().setInputCol(col).setOutputCol(col + \"_indexed\") for col in categorical_columns]\n",
    "encoders = [OneHotEncoder().setInputCol(col + \"_indexed\").setOutputCol(col + \"_encoded\") for col in categorical_columns]\n",
    "\n",
    "# Assemble all feature columns into a single vector\n",
    "encoded_feature_columns = [col + \"_encoded\" for col in categorical_columns]\n",
    "feature_columns = encoded_feature_columns + numerical_columns\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# Create a StringIndexer for the target column\n",
    "label_indexer = StringIndexer().setInputCol(\"person_gender\").setOutputCol(\"label\")\n",
    "\n",
    "# Combine all stages into a pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, label_indexer])\n",
    "\n",
    "# Fit and transform the data\n",
    "df_preprocessed = pipeline.fit(df).transform(df)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_df, test_df = df_preprocessed.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=100)\n",
    "\n",
    "# Train the model\n",
    "rf_model = rf.fit(train_df)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = rf_model.transform(test_df)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"Test set accuracy: {accuracy}\")\n",
    "print(f\"Test set precision: {precision}\")\n",
    "print(f\"Test set recall: {recall}\")\n",
    "print(f\"Test set F1-score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80ef268d-4896-4160-aa7a-cddf8677abc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Define the parameter grid\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.numTrees, [50, 100, 150])\n",
    "             .addGrid(rf.maxDepth, [5, 10, 15])\n",
    "             .build())\n",
    "\n",
    "# Define the cross-validator\n",
    "crossval = CrossValidator(estimator=rf,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_model = crossval.fit(train_df)\n",
    "cv_predictions = cv_model.transform(test_df)\n",
    "cv_accuracy = evaluator.evaluate(cv_predictions)\n",
    "\n",
    "print(f\"Cross-validated accuracy: {cv_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "696a4bcd-d2fd-449d-bc2a-19711e93de2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"Test set accuracy: {accuracy}\")\n",
    "print(f\"Test set precision: {precision}\")\n",
    "print(f\"Test set recall: {recall}\")\n",
    "print(f\"Test set F1-score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "785b6232-1bde-4263-9a61-7d26a39478f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44a4a5b4-d1cb-4331-899a-b07bb93c4e66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# impute rows with missing values\n",
    "df = df.fillna(0)\n",
    "\n",
    "# List of categorical columns to be encoded\n",
    "categorical_columns = [\"person_education\", \"person_home_ownership\", \"loan_intent\", \"previous_loan_defaults_on_file\", \"person_gender\"]\n",
    "\n",
    "# List of numerical columns\n",
    "numerical_columns = [\"person_age\", \"person_income\", \"person_emp_exp\", \"loan_amnt\", \"loan_int_rate\", \"loan_percent_income\", \"cb_person_cred_hist_length\", \"credit_score\"]\n",
    "\n",
    "for column in numerical_columns:\n",
    "    df = df.withColumn(column, col(column).cast(\"float\"))\n",
    "    \n",
    "# Create a StringIndexer and OneHotEncoder for each categorical column\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_indexed\") for col in categorical_columns]\n",
    "encoders = [OneHotEncoder(inputCol=col + \"_indexed\", outputCol=col + \"_encoded\") for col in categorical_columns]\n",
    "\n",
    "# Assemble all feature columns into a single vector\n",
    "encoded_feature_columns = [col + \"_encoded\" for col in categorical_columns]\n",
    "feature_columns = encoded_feature_columns + numerical_columns\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# Create a StringIndexer for the target column\n",
    "label_indexer = StringIndexer(inputCol=\"loan_status\", outputCol=\"label\")\n",
    "\n",
    "# Combine all stages into a pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, label_indexer])\n",
    "\n",
    "# Fit and transform the data\n",
    "df_preprocessed = pipeline.fit(df).transform(df)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_df, test_df = df_preprocessed.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=100)\n",
    "\n",
    "# Train the model\n",
    "rf_model = rf.fit(train_df)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = rf_model.transform(test_df)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"Test set accuracy: {accuracy}\")\n",
    "print(f\"Test set precision: {precision}\")\n",
    "print(f\"Test set recall: {recall}\")\n",
    "print(f\"Test set F1-score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5314fa4-6a76-418e-8239-e6d06e071d72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Define the parameter grid\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.numTrees, [50, 100, 150])\n",
    "             .addGrid(rf.maxDepth, [5, 10, 15])\n",
    "             .build())\n",
    "\n",
    "# Define the cross-validator\n",
    "crossval = CrossValidator(estimator=rf,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_model = crossval.fit(train_df)\n",
    "cv_predictions = cv_model.transform(test_df)\n",
    "\n",
    "\n",
    "cv_accuracy = evaluator.evaluate(cv_predictions)\n",
    "precision = evaluator.evaluate(cv_predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(cv_predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "f1 = evaluator.evaluate(cv_predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "\n",
    "print(f\"Cross-validated accuracy: {cv_accuracy}\")\n",
    "print(f\"CV set precision: {precision}\")\n",
    "print(f\"CV set recall: {recall}\")\n",
    "print(f\"CV set F1-score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ec3769e-3384-4bbc-89af-e70b21c12406",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"Test set accuracy: {accuracy}\")\n",
    "print(f\"Test set precision: {precision}\")\n",
    "print(f\"Test set recall: {recall}\")\n",
    "print(f\"Test set F1-score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "loan_acceptance",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "xai_dir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
